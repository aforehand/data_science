{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/alice/Code/pandas_helper')\n",
    "sys.path.append('/Users/alice/Code/metaheuristics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4a146664921d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0munivariate_analysis\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mua\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbivariate_analysis\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeature_selection\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/pandas_helper/univariate_analysis.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import univariate_analysis as ua\n",
    "import bivariate_analysis as ba\n",
    "import feature_selection as fs\n",
    "from ipynb.fs.full.genetic_local_search import GeneticLocalSearch\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_squared_log_error\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# adding missing revenue, budgets, and runtimes\n",
    "# from https://www.kaggle.com/enric1296/complete-guide-eda-feat-model\n",
    "train.loc[train['id'] == 16,'revenue'] = 192864          # Skinning\n",
    "train.loc[train['id'] == 90,'budget'] = 30000000         # Sommersby          \n",
    "train.loc[train['id'] == 118,'budget'] = 60000000        # Wild Hogs\n",
    "train.loc[train['id'] == 149,'budget'] = 18000000        # Beethoven\n",
    "train.loc[train['id'] == 313,'revenue'] = 12000000       # The Cookout \n",
    "train.loc[train['id'] == 451,'revenue'] = 12000000       # Chasing Liberty\n",
    "train.loc[train['id'] == 464,'budget'] = 20000000        # Parenthood\n",
    "train.loc[train['id'] == 470,'budget'] = 13000000        # The Karate Kid, Part II\n",
    "train.loc[train['id'] == 513,'budget'] = 930000          # From Prada to Nada\n",
    "train.loc[train['id'] == 797,'budget'] = 8000000         # Welcome to Dongmakgol\n",
    "train.loc[train['id'] == 819,'budget'] = 90000000        # Alvin and the Chipmunks: The Road Chip\n",
    "train.loc[train['id'] == 850,'budget'] = 90000000        # Modern Times\n",
    "train.loc[train['id'] == 1112,'budget'] = 7500000        # An Officer and a Gentleman\n",
    "train.loc[train['id'] == 1131,'budget'] = 4300000        # Smokey and the Bandit   \n",
    "train.loc[train['id'] == 1359,'budget'] = 10000000       # Stir Crazy \n",
    "train.loc[train['id'] == 1542,'budget'] = 10000000       # All at Once\n",
    "train.loc[train['id'] == 1542,'budget'] = 15800000       # Crocodile Dundee II\n",
    "train.loc[train['id'] == 1571,'budget'] = 4000000        # Lady and the Tramp\n",
    "train.loc[train['id'] == 1714,'budget'] = 46000000       # The Recruit\n",
    "train.loc[train['id'] == 1721,'budget'] = 17500000       # Cocoon\n",
    "train.loc[train['id'] == 1865,'revenue'] = 25000000      # Scooby-Doo 2: Monsters Unleashed\n",
    "train.loc[train['id'] == 2268,'budget'] = 17500000       # Madea Goes to Jail budget\n",
    "train.loc[train['id'] == 2491,'revenue'] = 6800000       # Never Talk to Strangers\n",
    "train.loc[train['id'] == 2602,'budget'] = 31000000       # Mr. Holland's Opus\n",
    "train.loc[train['id'] == 2612,'budget'] = 15000000       # Field of Dreams\n",
    "train.loc[train['id'] == 2696,'budget'] = 10000000       # Nurse 3-D\n",
    "train.loc[train['id'] == 2801,'budget'] = 10000000       # Fracture\n",
    "train.runtime[train.id == 391] = 86 #Il peor natagle de la meva vida\n",
    "train.runtime[train.id == 592] = 90 #А поутру они проснулись\n",
    "train.runtime[train.id == 925] = 95 #¿Quién mató a Bambi?\n",
    "train.runtime[train.id == 978] = 93 #La peggior settimana della mia vita\n",
    "train.runtime[train.id == 1256] = 92 #Cipolla Colt\n",
    "train.runtime[train.id == 1542] = 93 #Все и сразу\n",
    "train.runtime[train.id == 1875] = 86 #Vermist\n",
    "train.runtime[train.id == 2151] = 108 #Mechenosets\n",
    "train.runtime[train.id == 2499] = 108 #Na Igre 2. Novyy Uroven\n",
    "train.runtime[train.id == 2646] = 98 #同桌的妳\n",
    "train.runtime[train.id == 2786] = 111 #Revelation\n",
    "train.runtime[train.id == 2866] = 96 #Tutto tutto niente niente\n",
    "\n",
    "\n",
    "test.runtime[test.id == 4074] = 103 #Shikshanachya Aaicha Gho\n",
    "test.runtime[test.id == 4222] = 93 #Street Knight\n",
    "test.runtime[test.id == 4431] = 100 #Плюс один\n",
    "test.runtime[test.id == 5520] = 86 #Glukhar v kino\n",
    "test.runtime[test.id == 5845] = 83 #Frau Müller muss weg!\n",
    "test.runtime[test.id == 5849] = 140 #Shabd\n",
    "test.runtime[test.id == 6210] = 104 #Le dernier souffle\n",
    "test.runtime[test.id == 6804] = 145 #Chaahat Ek Nasha..\n",
    "test.runtime[test.id == 7321] = 87 #El truco del manco\n",
    "test.loc[test['id'] == 3889,'budget'] = 15000000       # Colossal\n",
    "test.loc[test['id'] == 6733,'budget'] = 5000000        # The Big Sick\n",
    "test.loc[test['id'] == 3197,'budget'] = 8000000        # High-Rise\n",
    "test.loc[test['id'] == 6683,'budget'] = 50000000       # The Pink Panther 2\n",
    "test.loc[test['id'] == 5704,'budget'] = 4300000        # French Connection II\n",
    "test.loc[test['id'] == 6109,'budget'] = 281756         # Dogtooth\n",
    "test.loc[test['id'] == 7242,'budget'] = 10000000       # Addams Family Values\n",
    "test.loc[test['id'] == 7021,'budget'] = 17540562       #  Two Is a Family\n",
    "test.loc[test['id'] == 5591,'budget'] = 4000000        # The Orphanage\n",
    "test.loc[test['id'] == 4282,'budget'] = 20000000       # Big Top Pee-wee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# more revenue corrections\n",
    "# from https://www.the-numbers.com/\n",
    "train.revenue.iloc[116] = 55287687\n",
    "train.revenue.iloc[150] = 18000000\n",
    "train.revenue.iloc[269] = 20018\n",
    "train.revenue.iloc[280] = 10155691\n",
    "train.revenue.iloc[347] = 72844\n",
    "train.revenue.iloc[639] = 10557291\n",
    "train.revenue.iloc[665] = 11514\n",
    "train.revenue.iloc[1138] = 15545115\n",
    "train.revenue.iloc[1161] = 272375\n",
    "train.revenue.iloc[1198] = 104268727\n",
    "train.revenue.iloc[1240] = 14683763\n",
    "train.revenue.iloc[1254] = 651\n",
    "train.revenue.iloc[1281] = 48977233\n",
    "train.revenue.iloc[1354] = 3900927\n",
    "train.revenue.iloc[1376] = 5245263\n",
    "train.revenue.iloc[1479] = 127257\n",
    "train.revenue.iloc[1754] = 1125910\n",
    "train.revenue.iloc[1800] = 131188\n",
    "train.revenue.iloc[1884] = 23693646\n",
    "train.revenue.iloc[1948] = 204612\n",
    "train.revenue.iloc[2117] = 3094699\n",
    "train.revenue.iloc[2251] = 51119758\n",
    "train.revenue.iloc[2255] = 6552255\n",
    "train.revenue.iloc[2433] = 32330354\n",
    "\n",
    "# from https://www.boxofficemojo.com/\n",
    "train.revenue.iloc[152] = 241278\n",
    "train.revenue.iloc[498] = 25317\n",
    "train.revenue.iloc[1141] = 13746\n",
    "train.revenue.iloc[1190] = 7660857\n",
    "train.revenue.iloc[1346] = 5000000\n",
    "train.revenue.iloc[2090] = 18369\n",
    "train.revenue.iloc[2474] = 23462"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# more runtime corrections\n",
    "# from https://www.omdbapi.com/\n",
    "\n",
    "train.runtime.iloc[390] = 86\n",
    "train.runtime.iloc[591] = 86\n",
    "train.runtime.iloc[1541] = 93\n",
    "train.runtime.iloc[1874] = 92\n",
    "train.runtime.iloc[2785] = 111\n",
    "train.runtime.iloc[2865] = 96\n",
    "\n",
    "test.runtime.iloc[1073] = 103\n",
    "test.runtime.iloc[1221] = 91\n",
    "test.runtime.iloc[1430] = 96\n",
    "test.runtime.iloc[2844] = 83\n",
    "test.runtime.iloc[2848] = 140\n",
    "test.runtime.iloc[3803] = 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log scaling\n",
    "train['budget_log'] = train.budget.apply(np.log1p)\n",
    "test['budget_log'] = test.budget.apply(np.log1p)\n",
    "\n",
    "train['revenue_log'] = train.revenue.apply(np.log1p)\n",
    "\n",
    "train['popularity_log'] = train.popularity.apply(np.log1p)\n",
    "test['popularity_log'] = test.popularity.apply(np.log1p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop useless columns\n",
    "train = train.drop(['imdb_id', 'poster_path', 'status'], axis = 1)\n",
    "test = test.drop(['imdb_id', 'poster_path', 'status'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill nans\n",
    "train.fillna('', inplace=True)\n",
    "test.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# feature generation - categorical\n",
    "\n",
    "train['has_homepage'] = train.homepage.apply(lambda x: x != '')\n",
    "test['has_homepage'] = test.homepage.apply(lambda x: x != '')\n",
    "\n",
    "train['has_tagline'] = train.tagline.apply(lambda x: x != '')\n",
    "test['has_tagline'] = test.tagline.apply(lambda x: x != '')\n",
    "\n",
    "train.release_date = pd.to_datetime(train.release_date)\n",
    "train_fix_years = train.release_date[train.release_date.dt.year > 2018]\n",
    "train_fix_years = train_fix_years.apply(lambda x: x.replace(x.year-100))\n",
    "train.release_date.update(train_fix_years)\n",
    "train['release_year'] = train.release_date.apply(lambda x: x.year)\n",
    "train['release_month'] = train.release_date.apply(lambda x: x.month)\n",
    "train['release_weekday'] = train.release_date.apply(lambda x: x.weekday())\n",
    "train['release_decade'] = train.release_year.apply(lambda x: (x//10)*10)\n",
    "test.release_date = pd.to_datetime(test.release_date)\n",
    "test_fix_years = test.release_date[test.release_date.dt.year > 2018]\n",
    "test_fix_years = test_fix_years.apply(lambda x: x.replace(x.year-100))\n",
    "test.release_date.update(test_fix_years)\n",
    "test['release_year'] = test.release_date.apply(lambda x: x.year)\n",
    "test['release_month'] = test.release_date.apply(lambda x: x.month)\n",
    "test['release_weekday'] = test.release_date.apply(lambda x: x.weekday())\n",
    "test['release_decade'] = test.release_year.apply(lambda x: (x//10)*10)\n",
    "for m in range(1,13):\n",
    "    train['release_month_'+str(m)] = train.release_month.apply(lambda x: x==m)\n",
    "    test['release_month_'+str(m)] = test.release_month.apply(lambda x: x==m)\n",
    "for d in range(7):\n",
    "    train['release_weekday_'+str(d)] = train.release_weekday.apply(lambda x: x==d)\n",
    "    test['release_weekday_'+str(d)] = test.release_weekday.apply(lambda x: x==d)\n",
    "\n",
    "# fill budget nans with median grouped by decade\n",
    "train.budget = train.groupby(train.release_decade).budget.apply(lambda x: x.replace(0, x.median()))\n",
    "test.budget = test.groupby(test.release_decade).budget.apply(lambda x: x.replace(0, x.median()))\n",
    "train['budget_log'] = train.budget.apply(np.log1p)\n",
    "test['budget_log'] = test.budget.apply(np.log1p)\n",
    "\n",
    "train['has_collection'] = train.belongs_to_collection.apply(lambda x: x!='')\n",
    "test['has_collection'] = test.belongs_to_collection.apply(lambda x: x!='')\n",
    "train.belongs_to_collection = ua.get_value_col(train.belongs_to_collection, key='id')\n",
    "test.belongs_to_collection = ua.get_value_col(test.belongs_to_collection, key='id')\n",
    "\n",
    "train['cast_size'] = train.cast.apply(ua.how_many)\n",
    "train['cast_freq'] = ua.get_freq_col(train.cast)\n",
    "train['fame_score'] = train.cast_freq/train.cast_size\n",
    "train.fame_score = train.fame_score.fillna(0)\n",
    "test['cast_size'] = test.cast.apply(ua.how_many)\n",
    "test['cast_freq'] = ua.get_freq_col(test.cast)\n",
    "test['fame_score'] = test.cast_freq/test.cast_size\n",
    "test.fame_score = test.fame_score.fillna(0)\n",
    "char_df = ua.get_freq_df(column = train.cast, key='character')\n",
    "for g in [0,1,2]:\n",
    "    train['cast_gender_'+str(g)] = train.cast.apply(lambda x: ua.get_value_count(x, key='gender', value=g))\n",
    "    train['cast_gender_'+str(g)] = train['cast_gender_'+str(g)]/train.cast_size\n",
    "    test['cast_gender_'+str(g)] = test.cast.apply(lambda x: ua.get_value_count(x, key='gender', value=g))\n",
    "    test['cast_gender_'+str(g)] = test['cast_gender_'+str(g)]/test.cast_size\n",
    "for c in char_df.value[:15]:\n",
    "    train['cast_character_'+c] = train.cast.apply(lambda x: c in x)\n",
    "    test['cast_character_'+c] = test.cast.apply(lambda x: c in x)\n",
    "    \n",
    "train['crew_size'] = train.crew.apply(ua.how_many)\n",
    "train['crew_director'] = ua.get_value_col(train.crew, k='job', v='Director')\n",
    "train['crew_producer'] = ua.get_value_col(train.crew, k='job', v='Producer')\n",
    "train['crew_executive_producer'] = ua.get_value_col(train.crew, k='job', v='Executive Producer')\n",
    "test['crew_size'] = test.crew.apply(ua.how_many)\n",
    "test['crew_director'] = ua.get_value_col(test.crew, k='job', v='Director')\n",
    "test['crew_producer'] = ua.get_value_col(test.crew, k='job', v='Producer')\n",
    "test['crew_executive_producer'] = ua.get_value_col(test.crew, k='job', v='Executive Producer')\n",
    "for g in [0,1,2]:\n",
    "    train['crew_gender_'+str(g)] = train.crew.apply(lambda x: ua.get_value_count(x, key='gender', value=g))\n",
    "    train['crew_gender_'+str(g)] = train['crew_gender_'+str(g)]/train.crew_size\n",
    "    test['crew_gender_'+str(g)] = test.crew.apply(lambda x: ua.get_value_count(x, key='gender', value=g))\n",
    "    test['crew_gender_'+str(g)] = test['crew_gender_'+str(g)]/test.crew_size\n",
    "crew_df = ua.get_freq_df(column=train.crew, key='name')\n",
    "for c in crew_df.value[:15]:\n",
    "    train['crew_name_'+c] = train.crew.apply(lambda x: c in x)\n",
    "    test['crew_name_'+c] = test.crew.apply(lambda x: c in x)\n",
    "job_df = ua.get_freq_df(column=train.crew, key='job')\n",
    "for j in job_df.value[:15]:\n",
    "    train['crew_job_'+c] = train.crew.apply(lambda x: c in x)\n",
    "    test['crew_job_'+c] = test.crew.apply(lambda x: c in x)\n",
    "dept_df = ua.get_freq_df(column=train.crew, key='department')\n",
    "for d in dept_df.value[:15]:\n",
    "    train['crew_dept_'+c] = train.crew.apply(lambda x: c in x)\n",
    "    test['crew_dept_'+c] = test.crew.apply(lambda x: c in x)\n",
    "    \n",
    "train['production_companies_size'] = train.production_companies.apply(ua.how_many)\n",
    "company_freq = ua.get_freq_col(train.production_companies)\n",
    "production_score = company_freq/train.production_companies_size\n",
    "train['production_score_log'] = production_score.apply(np.log1p)\n",
    "company_df = ua.get_freq_df(column=train.production_companies)\n",
    "test['production_companies_size'] = test.production_companies.apply(ua.how_many)\n",
    "company_freq = ua.get_freq_col(test.production_companies)\n",
    "production_score = company_freq/test.production_companies_size\n",
    "test['production_score_log'] = production_score.apply(np.log1p)\n",
    "for c in company_df.value[:10].values:\n",
    "    train['production_company_'+c] = train.production_companies.apply(lambda x: c in x)\n",
    "    test['production_company_'+c] = test.production_companies.apply(lambda x: c in x)\n",
    "\n",
    "train['production_countries_size'] = train.production_countries.apply(ua.how_many)\n",
    "test['production_countries_size'] = test.production_countries.apply(ua.how_many)\n",
    "country_df = ua.get_freq_df(column=train.production_countries)\n",
    "for c in country_df.value[:10].values:\n",
    "    train['production_country_'+c] = train.production_countries.apply(lambda x: c in x)\n",
    "    test['production_country_'+c] = test.production_countries.apply(lambda x: c in x)\n",
    "\n",
    "train['genres_size'] = train.genres.apply(ua.how_many)\n",
    "test['genres_size'] = test.genres.apply(ua.how_many)\n",
    "genre_df = ua.get_freq_df(column=train.genres)\n",
    "for genre in genre_df.value:\n",
    "    train['genre_'+genre] = train.genres.apply(lambda x: genre in x)\n",
    "    test['genre_'+genre] = train.genres.apply(lambda x: genre in x)\n",
    "    \n",
    "train['language_size'] = train.spoken_languages.apply(ua.how_many)\n",
    "train['english'] = train.original_language.apply(lambda x: x=='en')\n",
    "test['language_size'] = test.spoken_languages.apply(ua.how_many)\n",
    "train['english'] = train.original_language.apply(lambda x: x=='en')\n",
    "\n",
    "train['keyword_size'] = train.Keywords.apply(ua.how_many)\n",
    "test['keyword_size'] = test.Keywords.apply(ua.how_many)    \n",
    "keyword_df = ua.get_freq_df(column=train.Keywords)\n",
    "for k in keyword_df.value[:30].values:\n",
    "    train['keyword_'+k] = train.Keywords.apply(lambda x: k in x)\n",
    "    test['keyword_'+k] = test.Keywords.apply(lambda x: k in x)\n",
    "    \n",
    "# there are a lot of words, so choosing ones that have the most influence over revenue\n",
    "overview_important = ba.get_important_words(train.overview, train.revenue_log)\n",
    "# overview_important.value.describe()\n",
    "# count    54.000000\n",
    "# mean     27.703704\n",
    "# std      10.424263\n",
    "# min      20.000000\n",
    "# 25%      22.000000\n",
    "# 50%      24.000000\n",
    "# 75%      31.000000\n",
    "# max      90.000000\n",
    "\n",
    "for word in overview_important.value:\n",
    "    train['overview_'+word] = train.overview.apply(lambda x: word in x)\n",
    "    test['overview_'+word] = test.overview.apply(lambda x: word in x)\n",
    "    \n",
    "tagline_important = ba.get_important_words(train.tagline, train.revenue_log)\n",
    "# tagline_important.value.describe() \n",
    "# count    6.000000\n",
    "# mean     2.333333\n",
    "# std      0.516398\n",
    "# min      2.000000\n",
    "# 25%      2.000000\n",
    "# 50%      2.000000\n",
    "# 75%      2.750000\n",
    "# max      3.000000\n",
    "# tagline words are too sparse to have much effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# feature generation - numerical\n",
    "\n",
    "# fill numeric nans\n",
    "train.runtime = train.runtime.replace('',0)\n",
    "train.runtime = train.runtime.replace(0, train.runtime.median())\n",
    "test.runtime = test.runtime.replace('',0)\n",
    "test.runtime = test.runtime.replace(0, test.runtime.median())\n",
    "\n",
    "train.production_score_log.fillna(train.production_score_log.median(), inplace=True)\n",
    "test.production_score_log.fillna(test.production_score_log.median(), inplace=True)\n",
    "\n",
    "for g in [0,1,2]:\n",
    "    train['cast_gender_'+str(g)] = train['cast_gender_'+str(g)].fillna(train['cast_gender_'+str(g)].median())\n",
    "    test['cast_gender_'+str(g)] = test['cast_gender_'+str(g)].fillna(test['cast_gender_'+str(g)].median())\n",
    "    train['crew_gender_'+str(g)] = train['crew_gender_'+str(g)].fillna(train['crew_gender_'+str(g)].median())\n",
    "test['crew_gender_'+str(g)] = test['crew_gender_'+str(g)].fillna(test['crew_gender_'+str(g)].median())\n",
    "\n",
    "# feature selection\n",
    "\n",
    "# make sure numeric columns are actually numeric\n",
    "num_cols = train.select_dtypes(include=[np.float, np.int]).columns.tolist()\n",
    "# drop features from which log transforms have been generated\n",
    "num_cols.remove('budget')\n",
    "num_cols.remove('popularity')\n",
    "num_cols.remove('revenue_log')\n",
    "not_to_include = ['id',\n",
    " 'belongs_to_collection',\n",
    " 'revenue',\n",
    " 'release_month',\n",
    " 'release_weekday']\n",
    "train[not_to_include] = train[not_to_include].astype(str)\n",
    "\n",
    "# num_features = select_numeric_features(train, train.revenue_log)\n",
    "# bool_features = select_bool_features(train, train.revenue_log)\n",
    "\n",
    "bool_cols = train.select_dtypes(include=[np.bool]).columns.tolist()\n",
    "object_cols = train.select_dtypes(include=[np.object]).columns.tolist()\n",
    "\n",
    "train[bool_cols] = train[bool_cols].applymap(lambda x: 1 if x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = train[num_cols+bool_cols]\n",
    "target = pd.DataFrame(train.revenue_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale target\n",
    "target_scaler = StandardScaler()\n",
    "target_scaled = pd.DataFrame(target_scaler.fit_transform(target))\n",
    "# scale numeric data \n",
    "num_scaler = StandardScaler()\n",
    "train_num_scaled = train_num\n",
    "train_num_scaled[num_cols] = num_scaler.fit_transform(train_num[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean some more\n",
    "\n",
    "# drop rows with values > 3 standard deviations from the mean\n",
    "train_num_scaled['revenue_log'] = target_scaled\n",
    "\n",
    "train_num_scaled = train_num_scaled[target_scaled.values > -3]\n",
    "\n",
    "for col in num_cols:\n",
    "    if (train_num_scaled[col].values < -3).any() or (train_num_scaled[col].values > 3).any():\n",
    "        train_num_scaled = train_num_scaled[train_num_scaled[col].values > -3]\n",
    "        train_num_scaled = train_num_scaled[train_num_scaled[col].values < 3]\n",
    "        \n",
    "target_scaled = train_num_scaled['revenue_log']\n",
    "train_num_scaled.drop('revenue_log', axis=1, inplace=True)\n",
    "target_original = train.revenue[train.revenue.index.isin(target_scaled.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idk how that got back in there whatever\n",
    "train_num_scaled.drop('revenue', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_scaled.to_csv('target_scaled.csv')\n",
    "train_num_scaled.to_csv('train_num_scaled.csv')\n",
    "target_original.to_csv('target_original.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing finished, modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num_scaled_df = pd.read_csv('train_num_scaled.csv', index_col=0)\n",
    "target_scaled_df = pd.read_csv('target_scaled.csv', index_col=0, header=None)\n",
    "target_original = pd.read_csv('target_original.csv', index_col=0, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_scaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f6e7a3292299>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtarget_inverse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_scaled_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtarget_inverse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_inverse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'target_scaler' is not defined"
     ]
    }
   ],
   "source": [
    "target_inverse = target_scaler.inverse_transform(target_scaled_df)\n",
    "target_inverse = np.expm1(target_inverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing features based on f-test. much more reasonable output than using linreg\n",
    "\n",
    "f, p_val = f_regression(train_num_scaled_df, target_scaled_df)\n",
    "\n",
    "f_scores = zip(f, p_val, train_num_scaled_df.columns)\n",
    "f_scores = sorted(f_scores, reverse=True)\n",
    "\n",
    "f_importances_df = pd.DataFrame({'f': f, 'p_val': p_val, 'feature': train_num_scaled_df.columns})\n",
    "f_importances_df = f_importances_df.sort_values(by='f', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f_significant = train_num_scaled_df[f_importances_df['feature'][f_importances_df['p_val'] < 0.001]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6203528858030684"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linreg = LinearRegression().fit(train_f_significant, target_scaled_df)\n",
    "# linreg.score(train_f_significant, target_scaled_df)\n",
    "\n",
    "# pred = linreg.predict(train_f_significant)\n",
    "# math.sqrt(mean_squared_error(pred, target_scaled_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f_significant.to_csv('train_f_significant.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shouldn't have to touch anything above this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f_significant = pd.read_csv('train_f_significant.csv', index_col=1)\n",
    "target_scaled_df = pd.read_csv('target_scaled.csv', index_col=0, header=None)\n",
    "target_original = pd.read_csv('target_original.csv', index_col=0, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(train_f_significant, target_scaled_df)\n",
    "y_tr_original = target_original[target_original.index.isin(y_train.index)]\n",
    "y_val_original = target_original[target_original.index.isin(y_val.index)]\n",
    "\n",
    "# linreg.fit(x_train, y_train)\n",
    "# pred = linreg.predict(x_val)\n",
    "\n",
    "# pred_inv = target_scaler.inverse_transform(pred)\n",
    "# pred_inv = np.expm1(pred_inv)\n",
    "\n",
    "# math.sqrt(mean_squared_log_error(pred_inv, y_val_original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing population\n",
      "Training population\n",
      "....................................................................................................\n",
      "Generation 1:\n",
      "Selecting parents\n",
      "Making children\n",
      "Training population\n",
      "....................................................................................................\n",
      "Generation 2:\n",
      "Selecting parents\n",
      "Making children\n",
      "Training population\n",
      "....................................................................................................\n",
      "Generation 3:\n",
      "Selecting parents\n",
      "Making children\n",
      "Training population\n",
      "....................................................................................................\n",
      "Generation 4:\n",
      "Selecting parents\n",
      "Making children\n",
      "Training population\n",
      "....................................................................................................\n",
      "Generation 5:\n",
      "Selecting parents\n",
      "Making children\n",
      "Training population\n",
      "....................................................................................................\n",
      "Generation 6:\n",
      "Selecting parents\n",
      "Making children\n",
      "Training population\n",
      "....................................................................................................\n",
      "Generation 7:\n",
      "Selecting parents\n",
      "Making children\n",
      "Training population\n",
      "....................................................................................................\n",
      "Generation 8:\n",
      "Selecting parents\n",
      "Making children\n",
      "Training population\n",
      "....................................................................................................\n",
      "Generation 9:\n",
      "Selecting parents\n",
      "Making children\n",
      "Training population\n",
      "....................................................................................................\n",
      "Generation 10:\n",
      "Selecting parents\n",
      "Making children\n",
      "Training population\n",
      "....................................................................................................\n",
      "Generation 11:\n",
      "Selecting parents\n",
      "Making children\n",
      "Training population\n",
      "....................................................................................................\n",
      "Generation 12:\n",
      "Selecting parents\n",
      "Making children\n",
      "Training population\n",
      "....................................................................................................\n",
      "Generation 13:\n",
      "Selecting parents\n",
      "Making children\n",
      "Training population\n",
      "....................................................................................................\n",
      "Generation 14:\n",
      "Selecting parents\n",
      "Making children\n",
      "Training population\n",
      "....................................................................................................\n",
      "Generation 15:\n",
      "Selecting parents\n",
      "Making children\n",
      "Training population\n",
      "....................................................................................................\n",
      "Generation 16:\n",
      "Selecting parents\n",
      "Making children\n",
      "Training population\n",
      "....................................................................................................\n",
      "Generation 17:\n",
      "Selecting parents\n",
      "Making children\n",
      "Training population\n",
      "....................................................................................................\n",
      "Generation 18:\n",
      "Selecting parents\n",
      "Making children\n",
      "Training population\n",
      "..............................................."
     ]
    }
   ],
   "source": [
    "gls = GeneticLocalSearch()\n",
    "best_model = gls.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: get original target data. get scaler and reverse transform pred (exp(y)-1), calcualte rmsle\n",
    "# implement iterated local search; determine a good perturbation value\n",
    "# implement genetic local search\n",
    "# find good perturbation values\n",
    "# each new coef is a gaussian random value centered on the old coef \n",
    "# with sd some percentage of total distribution sd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
